{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving file names of product reviews...\n",
      "Finished...\n",
      "Total files:  25000\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "print(\"Retrieving file names of product reviews...\")\n",
    "\n",
    "reviews_files = ['product_reviews/' + f for f in listdir('product_reviews/') if isfile(join('product_reviews/', f))]\n",
    "num_files = len(reviews_files)\n",
    "\n",
    "print(\"Finished...\")\n",
    "print(\"Total files: \", num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextFromFiles(file_list):\n",
    "    reviews = []\n",
    "    for file in file_list:\n",
    "        with open(file, \"r\") as f:\n",
    "            text = f.read()\n",
    "            reviews.append(text) \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all reviews from files...\n",
      "Retrieved all text reviews from  25000 files\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading all reviews from files...\")\n",
    "reviews_dataset = getTextFromFiles(reviews_files)\n",
    "print(\"Retrieved all text reviews from \", num_files, \"files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great tutu and at a really great price. It doesn't look cheap at all. I'm so glad I looked on Amazon and found such an affordable tutu that isn't made poorly. A++\n",
      "\n",
      "I bought this for my 4 yr old daughter for dance class, she wore it today for the first time and the teacher thought it was adorable. I bought this to go with a light blue long sleeve leotard and was happy the colors matched up great. Price was very good too since some of these go for over $15.00 dollars.\n",
      "\n",
      "Wonder my niece wears it every single day, yellow is her favorite color right now an this cute little tutu made he da. It is well built and we hope she gets lots of wear out of it.\n",
      "\n",
      "It might just be me.  Although it seems well made and sized right.  It just seems a bit flimsy to me.  I debate if I really should not give it 5 stars as it does everything it advertises itself.  When I travel out of country, I have this around my neck and under one layer of clothing.  I have my &#34;throw away&#34; wallet in my pocket with expired cards and a few dollars.I like the various pockets and zippered security to keep things in place.  The neck cord is a little long so I had to tie it off so the stash would fall mid chest making it a bit more concealed.4.5 stars - minus 1/2 star for long cord and a flimsy feel.  Again though perhaps being flimsy is a good thing being under clothing making it conceal more.\n",
      "\n",
      "This bra is beautifully made and quite supportive. I'm large-busted (anywhere between a 32DDD and H, depending on brand/maker) and chose to try a 34DDD for this bra since cups run small on most brands that aren't specifically for large chests, and this item doesn't come larger than DDD. It turns out to be a little large in the cup, which almost never happens to me! But the support seems good and the bra is both delicate and sexy while still providing coverage. I'll be returning for a smaller size.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print out sample reviews from the datasets\n",
    "counter = 0\n",
    "for r in reviews_dataset:\n",
    "    print(r + \"\\n\" * 1)\n",
    "    counter += 1\n",
    "    if counter == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum of 100 per review\n",
    "def get_reviews(review, batch):\n",
    "    return \" \".join(review.split()[:batch]) \n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "num_of_words = 150\n",
    "\n",
    "def clean(doc):\n",
    "    doc = get_reviews(doc, num_of_words)\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning all reviews..\n",
      "\n",
      "Cleaned 5000 reviews..\n",
      "Cleaned 10000 reviews..\n",
      "Cleaned 15000 reviews..\n",
      "Cleaned 20000 reviews..\n",
      "\n",
      "Cleaning process finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning all reviews..\\n\")\n",
    "#cleaning the documents\n",
    "#doc_clean = [clean(doc).split() for doc in sample]\n",
    "\n",
    "cleaned = []\n",
    "counter = 0\n",
    "for doc in reviews_dataset:\n",
    "    cleaned.append(clean(doc).split())\n",
    "    if(counter % 5000 == 0 and counter != 0):\n",
    "        print(\"Cleaned\", counter, \"reviews..\")\n",
    "    counter += 1\n",
    "        \n",
    "print(\"\\nCleaning process finished.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ash\\Anaconda3\\envs\\tensorflow_1.1\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Importing Genism\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting text reviews to it's vector representation..\n",
      "Finished..\n"
     ]
    }
   ],
   "source": [
    "print(\"Converting text reviews to it's vector representation..\")\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index\n",
    "dictionary = corpora.Dictionary(cleaned)\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in cleaned]\n",
    "\n",
    "print(\"Finished..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatng the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model..\n",
      "LDA model is successfully trained.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the model..\")\n",
    "# Running and Training LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=15, id2word = dictionary, passes=100)\n",
    "\n",
    "print(\"LDA model is successfully trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the trained model to disk\n",
    "#save model\n",
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to disk.\n",
    "ldamodel.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a potentially pretrained model from disk.\n",
    "ldaModel = Lda.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, topic in ldaModel.print_topics(num_topics=15, num_words=8):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnMaxPrediction(predictions):\n",
    "    topic_id = 0\n",
    "    prediction = 0.00\n",
    "    for p in predictions:\n",
    "        id, p_weight = p\n",
    "        if(p_weight > prediction):\n",
    "            topic_id = id\n",
    "            prediction = p_weight\n",
    "    return topic_id, prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictTopics(text):\n",
    "    \n",
    "    test_doc_complete = [text]\n",
    "\n",
    "    #cleaning the documents\n",
    "    test_doc_clean = [clean(doc).split() for doc in test_doc_complete]\n",
    "\n",
    "    # Creating the term dictionary of our corpus, wheere every unique term is assigned an index\n",
    "    test_dictionary = corpora.Dictionary(test_doc_clean)\n",
    "\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above\n",
    "    test_doc_term_matrix = [test_dictionary.doc2bow(doc) for doc in test_doc_clean]\n",
    "    \n",
    "    for index, score in sorted(ldaModel[test_doc_term_matrix[0]], key=lambda tup: -1*tup[1]):\n",
    "        print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, ldaModel.print_topic(index, 10)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\n",
    "predictTopics(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_1.1",
   "language": "python",
   "name": "tensorflow_1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
